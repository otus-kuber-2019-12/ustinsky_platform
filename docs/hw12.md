
# Домашняя работа 6 (Debug)
## 1. kubectl debug

### 1.1 Установим kubectl debug по инструкции https://github.com/aylei/kubectl-debug
```        
$ export PLUGIN_VERSION=0.1.1

$ curl -Lo kubectl-debug.tar.gz \
    https://github.com/aylei/kubectl-debug/releases/download/v${PLUGIN_VERSION}/kubectl-debug_${PLUGIN_VERSION}_linux_amd64.tar.gz

$ tar -zxvf kubectl-debug.tar.gz kubectl-debug

$ sudo mv kubectl-debug /usr/local/bin/

# if your kubernetes version is v1.16 or newer
$ kubectl apply -f https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml

# if your kubernetes is old version(<v1.16), you should change the apiVersion to extensions/v1beta1, As follows
$ wget https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml
$ sed -i '' '1s/apps\/v1/extensions\/v1beta1/g' agent_daemonset.yml
$ kubectl apply -f agent_daemonset.yml
$ mv agent_daemonset.yml agent_daemonset_old.yml
```

### 1.2 Пробуем запустить strace и получаем ошибку 

Раньше (https://github.com/otus-kuber-2019-06/ustinsky_platform/tree/master/kubernetes-debug) мы получали ошибку. Мы меняли версию пода с 0.0.1 на latest, чтобы обойти проблему.
Но спустя какое то время в репозитории https://github.com/aylei/kubectl-debug сделали изменение
```git
+               CapAdd:      strslice.StrSlice([]string{"SYS_PTRACE", "SYS_ADMIN"}),
```
Теперь с манифестом по умолчанию все и так работает. И исправлять ничего не надо. 
Старый манифест теперь не работает на новых версиях kubernetes(на версии 1.15 работает, на 1.17 нет). Но обновленный работает на новом, на старом не работает. Текущая версия - v0.2.0-rc. Поэтому мы взяли старый манифест и зафиксировали версию на 0.1.1

В итоге в папке strace лежат 3 файла:
- 01-agent_daemonset-old.yml      - работает на 1.15, не работает на 1.17
- 01-agent_daemonset-default.yml  - работает на 1.17, не работает на 1.15. Это оригинальный файл https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml
- 01-agent_daemonset.yml          - работает на 1.15 и на 1.17

Все тестировалось на minikube.


#### 1.2.1 Запускаем пробный под
```
$ kubectl create ns test1
$ kubectl run nginx --image=nginx --port=80 -n test1 --generator=run-pod/v1
```

#### 1.2.2 Из другого терминала заходим kubectl debug-ом 
```
$ kubectl debug nginx -n test1 --port-forward
$ ps
PID   USER     TIME  COMMAND
    1 root      0:00 nginx: master process nginx -g daemon off;
   32 101       0:00 nginx: worker process
   51 root      0:00 bash
   56 root      0:00 ps
```
                
#### 1.2.3 Проверяем работу strace
```
bash-5.0# strace -p32 -c
strace: Process 32 attached
^Cstrace: Process 32 detached
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 27.38    0.000201         100         2           stat
 17.71    0.000130          65         2           writev
 11.72    0.000086          28         3           epoll_wait
 11.58    0.000085          42         2           openat
 10.08    0.000074          37         2           write
  6.95    0.000051          25         2           close
  5.18    0.000038          19         2           fstat
  4.77    0.000035          17         2           recvfrom
  2.45    0.000018          18         1           accept4
  1.23    0.000009           9         1           setsockopt
  0.95    0.000007           7         1           epoll_ctl
------ ----------- ----------- --------- --------- ----------------
100.00    0.000734          36        20           total
```

## 2 netperf-operator

### 2.0 Кластер
Работа производится на кластере k1s(респект maniaque) с изменным по умолчанию сетевым драйвером flannel => calico

### 2.1 Установка https://github.com/piontec/netperf-operator
```
$ git clone https://github.com/piontec/netperf-operator.git 
```

### 2.2 Запустить манифесты
```
$ kubectl apply -f ./deploy/crd.yaml
$ kubectl apply -f ./deploy/rbac.yaml
$ kubectl apply -f ./deploy/operator.yaml
```

### 2.3 Запустим пример
```
$ kubectl apply -f ./deploy/cr.yaml
$ kubectl describe netperf.app.example.com/example
...
Spec:
  Client Node:  
  Server Node:  
Status:
  Client Pod:          netperf-client-badc7f1ed8c1
  Server Pod:          netperf-server-badc7f1ed8c1
  Speed Bits Per Sec:  4016.32
  Status:              Done
Events:                <none>
```

### 2.4 Ставим политику (включаем логирование в iptables) и смотрим как изменился вывод
```
$ kubectl apply -f kit/netperf-calico-policy.yaml
$ kubectl delete -f netperf-operator/deploy/cr.yaml
$ kubectl apply -f netperf-operator/deploy/cr.yaml
$ kubectl describe netperf.app.example.com/example
...
  Client Node:  
  Server Node:  
Status:
  Client Pod:          netperf-client-297914915250
  Server Pod:          netperf-server-297914915250
  Speed Bits Per Sec:  0
  Status:              Started test
Events:                <none>
```
    
### 2.5 Получаем доступ SSH
     
Подключаемся к ноде по SSH и смотрим iptables

```
$ vagrant ssh k1s-master
$ sudo apt install iptables

$ kubectl logs pod/netperf-operator-57d59bc86b-2ckx8
time="2020-06-05T08:43:52Z" level=debug msg="New Netperf event, name: example, deleted: false, status: Done"
time="2020-06-05T08:43:52Z" level=debug msg="Nothing needed to do for update event on Netperf example in state Done"
time="2020-06-05T08:43:57Z" level=debug msg="New Netperf event, name: example, deleted: false, status: Done"
time="2020-06-05T08:43:57Z" level=debug msg="Nothing needed to do for update event on Netperf example in state Done"
time="2020-06-05T08:44:02Z" level=debug msg="New Netperf event, name: example, deleted: false, status: Done"
time="2020-06-05T08:44:02Z" level=debug msg="Nothing needed to do for update event on Netperf example in state Done"


Мы накатили исправленный вариант поэтому счетчики по нулям. При неисправленном варианте они ненулевые
$ vagrant ssh k1s-node
$ sudo iptables --list -nv | grep DROP - счетчики дропов 
$ sudo iptables --list -nv | grep LOG
$ sudo journalctl -k | grep calico
```

## 3 Запустим iptailer

### 3.1 Применим
```
$ kubectl apply -f kit/iptables-tailer.yaml 
$ kubectl describe daemonset kube-iptables-tailer -n kube-system
Events:
  Type     Reason        Age                From                  Message
  ----     ------        ----               ----                  -------
  Warning  FailedCreate  9s (x13 over 30s)  daemonset-controller  Error creating: pods "kube-iptables-tailer-" is forbidden: error looking up service account kube-system/kube-iptables-tailer: serviceaccount "kube-iptables-tailer" not found
```

### 3.2 Применим ServiceAccount
```
$ kubectl apply -f kit/kit-serviceaccount.yaml
$ kubectl apply -f kit/kit-clusterrole.yaml
$ kubectl apply -f kit/kit-clusterrolebinding.yaml 
$ kubectl describe daemonset kube-iptables-tailer -n kube-system
Events:
  Type     Reason            Age                   From                  Message
  ----     ------            ----                  ----                  -------
  Warning  FailedCreate      93s (x14 over 2m14s)  daemonset-controller  Error creating: pods "kube-iptables-tailer-" is forbidden: error looking up service account kube-system/kube-iptables-tailer: serviceaccount "kube-iptables-tailer" not found
  Normal   SuccessfulCreate  52s                   daemonset-controller  Created pod: kube-iptables-tailer-vj65n
  Normal   SuccessfulCreate  52s                   daemonset-controller  Created pod: kube-iptables-tailer-z8vsh
```

### 3.3 Пересоздадим netperf
```
$ kubectl delete -f netperf-operator/deploy/cr.yaml
$ kubectl apply -f netperf-operator/deploy/cr.yaml
$ kubectl describe netperf.app.example.com/example
Status:
  Client Pod:          netperf-client-d95b6e016ef3
  Server Pod:          netperf-server-d95b6e016ef3
  Speed Bits Per Sec:  3719.57
  Status:              Done
Events:                <none>
```
        
### 3.4 Проверяем
```
$ kubectl get events -A
kube-system   2m24s       Normal    Created            pod/kube-iptables-tailer-z8vsh    Created container kube-iptables-tailer
kube-system   2m23s       Normal    Started            pod/kube-iptables-tailer-z8vsh    Started container kube-iptables-tailer
kube-system   3m28s       Warning   FailedCreate       daemonset/kube-iptables-tailer    Error creating: pods "kube-iptables-tailer-" is forbidden: error looking up service account kube-system/kube-iptables-tailer: serviceaccount "kube-iptables-tailer" not found
kube-system   2m47s       Normal    SuccessfulCreate   daemonset/kube-iptables-tailer    Created pod: kube-iptables-tailer-vj65n
kube-system   2m47s       Normal    SuccessfulCreate   daemonset/kube-iptables-tailer    Created pod: kube-iptables-tailer-z8vsh

$ kubectl get pod 
NAME                                READY   STATUS             RESTARTS   AGE
netperf-client-c66ed5074ffc         0/1     CrashLoopBackOff   6          21m

$ kubectl describe pod/netperf-client-c66ed5074ffc
Name:         netperf-client-c66ed5074ffc
Namespace:    default
Priority:     0
Node:         k1s-node/192.168.33.120
Start Time:   Fri, 05 Jun 2020 09:56:46 +0000
Labels:       app=netperf-operator
              netperf-type=client
Annotations:  cni.projectcalico.org/podIP: 10.244.146.141/32
              cni.projectcalico.org/podIPs: 10.244.146.141/32
Status:       Running
IP:           10.244.146.141
IPs:
  IP:           10.244.146.141
Controlled By:  Netperf/example
Containers:
  netperf-client-c66ed5074ffc:
    Container ID:  docker://23873c3a95b92f3e5d77d8ef5c2f53b1ade5d36ba353df6b6e98d3fb2650d638
    Image:         tailoredcloud/netperf:v2.7
    Image ID:      docker-pullable://tailoredcloud/netperf@sha256:0361f1254cfea87ff17fc1bd8eda95f939f99429856f766db3340c8cdfed1cf1
    Port:          <none>
    Host Port:     <none>
    Command:
      netperf
      -H
      10.244.146.140
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Fri, 05 Jun 2020 10:15:40 +0000
      Finished:     Fri, 05 Jun 2020 10:17:50 +0000
    Ready:          False
    Restart Count:  6
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hrnjc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-hrnjc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-hrnjc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason      Age                  From                  Message
  ----     ------      ----                 ----                  -------
  Normal   Scheduled   <unknown>            default-scheduler     Successfully assigned default/netperf-client-c66ed5074ffc to k1s-node
  Normal   Pulled      12m (x5 over 22m)    kubelet, k1s-node     Container image "tailoredcloud/netperf:v2.7" already present on machine
  Normal   Created     12m (x5 over 22m)    kubelet, k1s-node     Created container netperf-client-c66ed5074ffc
  Normal   Started     12m (x5 over 22m)    kubelet, k1s-node     Started container netperf-client-c66ed5074ffc
  Warning  PacketDrop  3m45s (x6 over 20m)  kube-iptables-tailer  Packet dropped when sending traffic to server (10.244.146.140)
  Warning  BackOff     95s (x28 over 18m)   kubelet, k1s-node     Back-off restarting failed container
```

### 3.5 Поправьте файлы
Все измененные файлы лежат в репозитории в папке kit
